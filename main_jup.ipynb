{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0c6ad9",
   "metadata": {},
   "source": [
    "# Replication of Code \n",
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af2a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51f642",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "This is a preprocessing function that will extract the data and the features from the full dataset. Also it will perform min-max normalization by column as defined in the paper and encode the output class. The it will create two pytorch tensors with a window of 32 elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3d2689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset has shape:  (94380, 52)\n",
      "nan count in data False\n",
      "nan count in data_buffer False\n",
      "data tensor are of type:  <class 'torch.Tensor'> shape:  torch.Size([94380, 52])\n",
      "encoded label are of type:  <class 'torch.Tensor'> shape:  torch.Size([94380, 10])\n",
      "data normalized and windowed are of type:  <class 'torch.Tensor'> shape:  torch.Size([5898, 32, 52])\n",
      "label normalized and windowed are of type:  <class 'torch.Tensor'> shape:  torch.Size([5898, 32, 10])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "# Dropping useless features and separate labels from data\n",
    "data = df.drop(columns=['Class','PathOrder'])\n",
    "label = df.Class.to_numpy()\n",
    "print(\"the dataset has shape: \", data.shape)\n",
    "\n",
    "print(\"nan count in data\", data.isnull().values.any())\n",
    "\n",
    "# Normalize each column(feature) of the dataframe \n",
    "\n",
    "# A buffer to fill with the normalized values of the dataframe \n",
    "data_buffer = pd.DataFrame().reindex_like(data)\n",
    "\n",
    "for colname, colval in data.iteritems():\n",
    "    #get min and max values of the current column \n",
    "    col_min = data[colname].min() \n",
    "    col_max = data[colname].max() \n",
    "        \n",
    "    #check if I there is a division by zero in denominator \n",
    "    if (col_max - col_min > 0):\n",
    "        # I can do the normalization \n",
    "        data_buffer[colname] = (data[colname] - col_min)/(col_max-col_min)\n",
    "    else:\n",
    "        # Set the normalized value to 0 \n",
    "        data_buffer[colname] = 0\n",
    "\n",
    "print(\"nan count in data_buffer\", data_buffer.isnull().values.any())\n",
    "\n",
    "# Transform the normalized dataframe into a torch tensor \n",
    "data_norm_tensor = torch.tensor(data_buffer.values)\n",
    "print(\"data tensor are of type: \", type(data_norm_tensor), \"shape: \", data_norm_tensor.size())\n",
    "\n",
    "\n",
    "#One-Hot Encode the label column of dataframe and keep it as a pytorch vector    \n",
    "targets = preprocessing.LabelEncoder().fit_transform(label)\n",
    "targets = torch.as_tensor(targets)\n",
    "label_encoded_tensor = torch.nn.functional.one_hot(targets, num_classes = 10)\n",
    "print(\"encoded label are of type: \", type(label_encoded_tensor), \"shape: \", label_encoded_tensor.size())\n",
    "\n",
    "# At this point I have the normalized dataframe and the encoded labels in a tensor format \n",
    "# I need to create a windowed tensor with window_size \n",
    "\n",
    "window_size = 32 \n",
    "step = int(window_size/2)\n",
    "\n",
    "data_norm_tensor_win = torch.empty((len(range(0,len(data_buffer)-step,step)),window_size,data.shape[1]), dtype=torch.float32)\n",
    "label_encoded_tensor_win = torch.empty((len(range(0,len(label_encoded_tensor)-step,step)),window_size,10), dtype=torch.float32)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for i in range(0,len(data_buffer)-2*step,step):  \n",
    "    data_norm_tensor_win[counter] = data_norm_tensor[i:i+window_size][:]\n",
    "    label_encoded_tensor_win[counter] = label_encoded_tensor[i:i+window_size][:]\n",
    "    \n",
    "    counter=counter+1      \n",
    "\n",
    "print(\"data normalized and windowed are of type: \", type(data_norm_tensor_win), \"shape: \", data_norm_tensor_win.size())    \n",
    "print(\"label normalized and windowed are of type: \", type(label_encoded_tensor_win), \"shape: \", label_encoded_tensor_win.size())    \n",
    "\n",
    "\n",
    "\n",
    "# I now have the pytorch tensors windowed to feed in the network \n",
    "# data_norm_tensor_win -> data\n",
    "# label_encoded_tensor_win -> label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67318895",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nan_to_num(data_norm_tensor_win)\n",
    "print(torch.isnan(data_norm_tensor_win).any())\n",
    "torch.isnan(data_norm_tensor_win)\n",
    "print(\" d\",torch.isnan(data_norm_tensor_win.view(-1)).sum().item()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np[5897][6][36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ea842",
   "metadata": {},
   "source": [
    "## Pytorch dataset and dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e970e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "dataset = CustomDataset(data_norm_tensor_win,label_encoded_tensor_win)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849572f3",
   "metadata": {},
   "source": [
    "## Pytorch network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURAL(\n",
      "  (lstm1): LSTM(52, 160, dropout=0.5)\n",
      "  (lstm2): LSTM(160, 200, dropout=0.5)\n",
      "  (fc): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (logsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "Train Epoch: 0 [0/4718 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 0 [640/4718 (14%)]\tLoss: 11.090357\n",
      "Train Epoch: 0 [1280/4718 (27%)]\tLoss: 11.090356\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "class NEURAL(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NEURAL, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(52, 160, 1,dropout = 0.5,batch_first = False)\n",
    "        self.lstm2 = torch.nn.LSTM(160, 200, 1,dropout = 0.5)\n",
    "        self.fc = torch.nn.Linear(200, 10)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.logsoftmax=torch.nn.LogSoftmax()\n",
    "       \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size(0))\n",
    "        x = torch.nan_to_num(x,nan=0.0)\n",
    "        #print(x.shape)\n",
    "        if(x.isnan().any()): \n",
    "            print(\"found a nan\")\n",
    "            x = torch.nan_to_num(x,nan=0.0)\n",
    "            \n",
    "        #print(\"is nan x\", x.isnan().any() ) \n",
    "        h_t1 = Variable(torch.zeros(1, x.size()[1], 160))\n",
    "        c_t1 = Variable(torch.zeros(1, x.size()[1], 160))\n",
    "        h_t2 = Variable(torch.zeros(1, x.size()[1], 200))\n",
    "        c_t2 = Variable(torch.zeros(1, x.size()[1], 200))\n",
    "        \n",
    "        h1, (h_t1, c_t1) = self.lstm1(x, (h_t1, c_t1))\n",
    "        \n",
    "    \n",
    "        #print(\"is nan h1\", h1.isnan().any() ) \n",
    "        \n",
    "        h2, (h_t2, _) = self.lstm2(h1, (h_t2, c_t2))\n",
    "        \n",
    "        #print(\"is nan h2\", h2.isnan().any() ) \n",
    "        \n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "\n",
    "\n",
    "        h3 = self.fc(h2)\n",
    "        #print(\"is nan h3\", h3.isnan().any() ) \n",
    "        h4 = self.sigmoid(h3)\n",
    "        #print(\"is nan h4\", h4.isnan().any() ) \n",
    "        \n",
    "        return h4\n",
    "    \n",
    "model = NEURAL()\n",
    "print(model)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 500\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.requires_grad_()\n",
    "       # print(\"data\",data)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(data)\n",
    "        #print(\"outputs\",outputs.shape)\n",
    "        #print(\"labels\",labels.shape)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(data), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9911cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Plot an histogram with number of timesteps for each class \n",
    "df['Class'].value_counts().plot(kind='bar', title='Number of data point per class',color='C1')\n",
    "plt.ylabel('Data Points')\n",
    "plt.xlabel('Classes')\n",
    "\n",
    "data_norm, label =  preporcess(df)\n",
    "\n",
    "# sanity c\n",
    "#for colname, colval in data_norm.iteritems():\n",
    "#    print(colname,\" max : \",colval.max(),\"min : \",colval.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ccccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
