{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd29beee",
   "metadata": {},
   "source": [
    "# Reproduction of \"Driver Identiﬁcation Based on Vehicle Telematics Data using LSTM-Recurrent Neural Network\" using pytorch\n",
    "\n",
    "Author: Lorenzo Pes (5604958)\n",
    "\n",
    "The main objective of this jupyter notebook is to provide a replication of the paper [Driver Identiﬁcation Based on Vehicle Telematics Data using LSTM-Recurrent Neural Network](https://arxiv.org/abs/1911.08030) using PYtorch to create the neural model proposed. The main objective of the paper is to train a model which can identify the driver of a car based on his driving behaviour. This allow the model to identify in real-time if the driver of the car is the owner or someone else. To achieve this goal the paper attempts to train a Recurrent Neural Network (RNN) with LSTM cells, using sensors data obtained from OBD-II. The dataset is comprised of measurements such as vehicle speed, throttle position, break pedal displacement etc. Specifically, this notebooks attempts to reproduce the LSTM results of figure 9 and 10 of the paper shown below. \n",
    "\n",
    "![alt text](f9.png \"Figure 9\") \n",
    "<center>Figure 9 </center>\n",
    "\n",
    "In the Figure 9, the authors of the paper investigated how different levels of white gaussian noise in the input sensors data would affect the accuracy of a trained model. In addition to that, the autorhs compare the results of their LSTM model with other algorithms such as FCNN, Decision Tree and Random Forest demostrating that the LSTM network is the most insensitive to noisy sensor data. \n",
    "\n",
    "![alt text](f10.png \"Figure 10\") \n",
    "<center>Figure 10 </center>\n",
    "\n",
    "On the other end, in Figure 10, the authors demonstrate the accuracy on the test set for the same algorithms as before, but with the model trained with noisy data. Even for this set-up the LSTM shows the better accuracy against its competitors. \n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The authors test their model with two different datasets: Security Driving Dataset and Vehicular data trace Dataset-1. Unfortunatelly, due to these datasets not being publicly available, another similar [dataset](https://ocslab.hksecurity.net/Datasets/driving-dataset) was used. It consists of recorded sensor data from 10 different drivers of a trip of about 46km, between Korea University and SANGAM World Cup Stadium. Let's start investigating the dataset with pandas. \n",
    "\n",
    "### Inport the functions required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667440d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32365ee",
   "metadata": {},
   "source": [
    "I am now going to load the dataset and display some useful informations. Lets first display the name of all the columns of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c9f01",
   "metadata": {},
   "source": [
    "### Features name of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2561224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "for col_name in df.columns: \n",
    "    print(col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d194a",
   "metadata": {},
   "source": [
    "### Sensor recordings per driver class \n",
    "I will not plot the number of recordings for each driver in a histogram fromat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts().sort_index().plot(kind='bar', title='Recordings vs Driver')\n",
    "plt.ylabel('Recordings')\n",
    "plt.xlabel('Driver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac41aa2",
   "metadata": {},
   "source": [
    "The Figure above shows the number of recordings for each driver. It can be noticed that in this dataset, the number of recorded sensor data is not equivalent for each driver. For instance, drive B and D posses one order of magnitude more data that the avarage of the other drivers. In the next cells I will print the histogram for each column of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9fb95",
   "metadata": {},
   "source": [
    "### Histogram of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74603bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df.hist(bins=100,figsize=(36,20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0ef2f",
   "metadata": {},
   "source": [
    "From the figure above, it can be seen that some recording are constantly zero. For istance the measurements of **Filtered_Accelerator_Pedal_value, Inhibition_of_engine_fuel_cut_off, Fuel_Pressure, Torque_scaling_factor(standardization) and Glow_plug_control_request** are constantly zero. Consequently, these values can be dropped out from the dataset when training the network because input features with all zeors will be insignificant for the training of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fc8e8",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "In this section of the notebook, it will be explained how I have pre-processed the dataset to feed in the network. Clearly, this step is based on the description provided in the paper. However, some ambiguities were found in the author description and they will be further investigated in the next sections. To begin with, the range of sensors recordings are not equal (i.e **engine_cooling_temperature** measurements are between 75 and 100, while **torque_converter_speed** measurements are between 0 and 3000). This umbalance of ranges will make the **torque_converter_speed** recordings more influential than **engine_cooling_temperature** due to its larger range. Furthermore, the authors frame the problem as a time-series prediction. That is, a temporal series of sensors features is fed to the input of Recurrent Neural Networks and predicts the deriver of that sequence. To achieve this goal, the dataset must be split into multiple windows as explained further in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f424953",
   "metadata": {},
   "source": [
    "### Normalization Functions\n",
    "For the training of the dataset, we would like all the sensors readings to have the same influence over the network. To achieve the former, the authors states in the paper that the dataset is standardized using min-max normalization. Unfortunately, this is not proven to be the same in the code provided by the authors. For instance, by looking at the function $normalize()$ in [provided code](https://github.com/Abeni18/Deep-LSTM-for-Driver-Identification-/blob/master/main.py) it seems that the data is actually normalized using a $StandardScaler()$ rather than a min-max standardization. Consequently, in this reproduction both normalization techniques are implemented to study how the affect the resulting accuracy of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9ad62",
   "metadata": {},
   "source": [
    "#### Min-Max Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(self,col):\n",
    "    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acefab1",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_scaler(self,col):\n",
    "    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b5ab9",
   "metadata": {},
   "source": [
    "### Windowing and Pytorch Dataloader\n",
    "As previously mentioned, the input of the recurrent network is a sequence of time instances containing all the recorded features. Figure 5 taken from the paper gives and example of the former.\n",
    "\n",
    "![alt text](window.png \"Figure 5\") \n",
    "<center>Figure 5 </center>\n",
    "\n",
    "It can be seen that the the windows are overlapping by 50% to smooth the flow of sequence. Furthermore, the authors state that after hyperparameter tuning it was found that the optimal windows size consists of 16 consecutive time instances. Since the number of measurements for each class is not the same and the number of recordings for a given class might not be exactly divisible by the window size, some recording were dropped to avoid that a temporal sequence might contains values from another driver (class). Sadly, the authors do not elaborate in this regard and therefore this intuition might not be exatly what is done in the implementation, but it is still a safe assumption to make. Perhaps, by looking at the code implementation, it seems that the authors do not perform this dropping but rather choose the most occuring driver for a griven temporal sequence. However, the results should not be affected by this difference in framing the dataset. Also, the output class must be encoded into a numerical values to be understood by the network. Both functions have been implemented inside the constructor of a pytorch dataloader as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb53f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path='dataset.csv', classes_to_drop=['Class', \n",
    "                                                                 'PathOrder', \n",
    "                                                                 'Time(s)', \n",
    "                                                                 'Filtered_Accelerator_Pedal_value',\n",
    "                                                                 'Inhibition_of_engine_fuel_cut_off',\n",
    "                                                                 'Fuel_Pressure',\n",
    "                                                                 'Torque_scaling_factor(standardization)',\n",
    "                                                                 'Glow_plug_control_request'], window_size=16, normalize=True, normalize_method='mean_std'):\n",
    "        \n",
    "        \n",
    "        self._window_size=window_size\n",
    "        self._data=pd.read_csv(file_path)\n",
    "        \n",
    "        # The data is sorted by Class A,B,C the indexes of the dataframe have restarted by ignore index\n",
    "        self._data = self._data.sort_values(by=['Class'], inplace=False,ignore_index = True) \n",
    "        \n",
    "        # class_uniq contains the letters of the drivers A,B and it loops across all of them \n",
    "        for class_uniq in list(self._data['Class'].unique()):\n",
    "            # find the total number of elements belonging to a class\n",
    "            tot_number=sum(self._data['Class']==class_uniq) \n",
    "            #number of elements to drop so that the class element is divisible by window size \n",
    "            to_drop=tot_number%window_size\n",
    "            #returns the index of the first element of the class\n",
    "            index_to_start_removing=self._data[self._data['Class']==class_uniq].index[0]\n",
    "            #drop element from first element to the element required \n",
    "            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n",
    "            \n",
    "        \n",
    "        #resetting index of dataframe after dropping values \n",
    "        self._data = self._data.reset_index()\n",
    "        self._data = self._data.drop(['index'], axis=1)\n",
    "        \n",
    "        index_starting_class=[] # This array contains the starting index of each class in the df\n",
    "        for class_uniq in list(self._data['Class'].unique()):\n",
    "            #Appending the index of first element of each clas\n",
    "            index_starting_class.append(self._data[self._data['Class']==class_uniq].index[0])\n",
    "        \n",
    "        # Create the sequence of indexs of the windows \n",
    "        sequences=[]\n",
    "        for i in range(len(index_starting_class)):\n",
    "            #check if beginning of next class is there \n",
    "            if i!=len(index_starting_class)-1:\n",
    "                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])       \n",
    "            for j in range(0,len(ranges),int(self._window_size/2)):\n",
    "                if len(ranges[j:j+self._window_size])==16:\n",
    "                    sequences.append(ranges[j:j+self._window_size])\n",
    "        self._sequences=sequences\n",
    "    \n",
    "        \n",
    "        #take only the 'Class' which are the actual labels and store it in the labels of self \n",
    "        self._labels=self._data['Class']\n",
    "        # Dropping columns which have constant measurements because they would return nan in std\n",
    "        self._data.drop(classes_to_drop, inplace=True, axis=1)\n",
    "        \n",
    "        # Function to normalize the data either with min_max or mean_std\n",
    "        if normalize:\n",
    "            for col in self._data.columns:\n",
    "                if normalize_method=='min_max':\n",
    "                    min_max_norm(self,col)\n",
    "                elif normalize_method==\"mean_std\":\n",
    "                    std_scaler(self,col)\n",
    "                    \n",
    "        # Create the array holding the windowed multidimensional arrays \n",
    "        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n",
    "        y=[]\n",
    "        \n",
    "        for n_row, sequence in enumerate(sequences):\n",
    "            X[n_row,:,:]=self._data.iloc[sequence]\n",
    "            # I am saying that the corresponding driver of the sequence is the driver at first sequence\n",
    "            # This should be OK but not sure, maybe take the most recurrent driver in the sequence ? \n",
    "            y.append(self._labels[sequence[0]]) \n",
    "            #y.append(self._labels.iloc[sequence])\n",
    "       \n",
    "\n",
    "        assert len(y)==len(X)\n",
    "        #Assing the windowed dataset to the X of self \n",
    "        self._X= X\n",
    "        \n",
    "        # targets is a transformed version of y with drivers are encoded into 0 to 9 \n",
    "        targets = preprocessing.LabelEncoder().fit_transform(y)\n",
    "        targets = torch.as_tensor(targets) # just converting it to a pytorch tensor\n",
    "        self._y=targets # assign it to y of self \n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._X)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2c41e",
   "metadata": {},
   "source": [
    "At this point we have a fully customized dataloader for pytorch which will allow us to create the dataloaders for the traning, validation and test sets. Lets start first my creating an instance of the custom dataloader as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CustomDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0fc5e",
   "metadata": {},
   "source": [
    "### Train, Validation and Test set\n",
    "\n",
    "The authors of the papers splits the dataset in the following way:\n",
    "\n",
    "* 85% training  \n",
    "* 5%  validation\n",
    "* 10% test \n",
    "\n",
    "The same will be done in our replication as shown in figure below. Also, the optimal batch size found by the authors will be initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30719861",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(a))\n",
    "val_size = int(0.05 * len(a))\n",
    "test_size = len(a)-train_size-val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(a, [train_size,val_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    " \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False,\n",
    "                                           drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df51b6a",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "In this section of the notebook the implmentation of the Recurrent Neural Network with LSTM cells will be explained. To begin with, the paper implements a many-to-one RNN two hidden layers composed of LSTM cells. Figure 3 taken form the paper, provides a visual representation of the network. \n",
    "\n",
    "![alt text](net.png \"Figure 3\") \n",
    "<center>Figure 3 </center>\n",
    "\n",
    "As it can be seen in the image, a temporal sequence and all the relative sensors data are fed into the first hidden layer, above which, a second hidden layer is stacked. In the papers it is stated that \"the last hiddel layer feature vector is taken and used to output a classification score for the given input sequence\". Understanding this part is of fundamental importance for the correct reproduction of the paper. Nevertheless, the authors spend no more that one paragraph in this topic and provide an image that does not really clarify how the output classification is performed. Furthermore, the authors state that \"the last layer of the is a sigmoid function\". It is really unclear what is meant by the former sentence because the sigmoid function is just an activation function of a given layer and not the layer itself. Perhaps, the last layer is actually a fully connected layer with an output dimension of 10, one for each class. In the replication of the code, the last layer is considered as as fully connected linear layer with a sigmoid activation function at its output. \n",
    "\n",
    "Another important aspect to be taken into account for the declaration of the model is the number of neurons per hidden layer. After hyper-parameter tuning, the authors found the best number of neurons to be **160 for the first hidden** layer and **200 for the second hidden layer**. Furthermore, to ease the training process batch normalization has been indroduced at the output of each LSTM hidden layer. The class implementing the network is shown in the cell below. \n",
    "\n",
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEURAL(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, window_size, num_features):\n",
    "        super(NEURAL, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(num_features, 160,batch_first = True)\n",
    "        #self.bn1 = torch.nn.BatchNorm1d(16)\n",
    "        self.lstm2 = torch.nn.LSTM(160, 200, 1)\n",
    "        #self.bn2 = torch.nn.BatchNorm1d(16)     \n",
    "        self.fc = torch.nn.Linear(200, 10)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm1_out, (h_t1, c_t1) = self.lstm1(x)  \n",
    "        #self.bn1(lstm1_out)\n",
    "        lstm2_out, (h_t2, c_t2) = self.lstm2(lstm1_out)  \n",
    "        #self.bn2(lstm2_out)\n",
    "        fc_out = self.fc(lstm2_out)\n",
    "        out = self.sigmoid(fc_out)\n",
    "        \n",
    "        return out[:,-1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bb6e7",
   "metadata": {},
   "source": [
    "Now lets create an instance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0acb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(train_loader)) \n",
    "model = NEURAL(inputs.shape[0],inputs.shape[1],inputs.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db65d2df",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "As specified by the paper, the loss function used to train the network is the normal Cross Entropy Loss. On the other end, the authors  do not specify the learning rate nor the number of epochs nor the optimizer. In the reproduction of the paper the learning rate has been initialized with a value of 0.001, but set to decay exponentially with the number of epochs as defined by the function *lambda1*. Furthermore, Adam optimizer was choosen even if better results on the test set might be achivable with SGD. Also, for the number of epochs, different test were performed to find a number of epochs equal to x that leads to an F1 score similar to the one provided by the authors. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lambda1 = lambda epoch: 0.99 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    running_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(data)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        # Compute the loss and its gradients\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "       \n",
    "    \n",
    "    print('Loss Avg: {:.6f}'.format(running_loss/len(train_loader)))\n",
    "            \n",
    "    return running_loss/len(train_loader)\n",
    "\n",
    "epoch_number = 0\n",
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train(True)\n",
    "    train_loss.append(train_one_epoch(epoch))\n",
    "    scheduler.step()\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063630b",
   "metadata": {},
   "source": [
    "In the next cell I will just save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'noiseless_model/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37155c",
   "metadata": {},
   "source": [
    "### Average Loss vs Epochs Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a47fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bee3e",
   "metadata": {},
   "source": [
    "### Metrics \n",
    "The paper uses F1-score to measure the performance of the model in the test set. This metric is defined by the following set of equations:\n",
    "\n",
    "$$ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}  $$\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}  $$\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+TN}  $$\n",
    "\n",
    "In the equations above the variables have the following meanings: \n",
    "\n",
    "* TP: True Positive \n",
    "* FP: False Positive \n",
    "* FN: False Negative \n",
    "\n",
    "The F1 scores can be computed for each class of the drivers. From the papers it can be deduced that the total F1 score used to measure the accuracy of the model is the average of each class. Fortunatelly, scikit-learn provides a function to calculate the F1 score for each class given a vector of true and predicted labels. The function to calculate the accuracy of the model is shown in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "inputs, classes = next(iter(train_loader)) \n",
    "model_noiseless = NEURAL(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
    "model_noiseless.load_state_dict(torch.load('noiseless_model/model.pt'))\n",
    "model_noiseless.eval()\n",
    "\n",
    "\n",
    "\n",
    "def f1(test_loader,model):\n",
    "    f1 = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            outputs = model(data)\n",
    "            pred = outputs.data.max(1, keepdim=True)[1] \n",
    "            f1 += f1_score(labels, pred, average='macro')\n",
    "        \n",
    "    avg_f1 = f1/len(test_loader) \n",
    "    \n",
    "    return(avg_f1*100)\n",
    "\n",
    "f1(test_loader,model_noiseless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc740b",
   "metadata": {},
   "source": [
    "##  Noise \n",
    "\n",
    "In this part of the notebook the effect of adding noise with different standard deviations to the sensor readings will be investigated. Frist, the accuracy of the model trained with noiseless data will be calculated for different level of noise added to the input vectors. After, a new model with noisy input data will be trained to calculate how much the accuracy degrades and if there is improvements. \n",
    "\n",
    "The authors are slightly unclear on their definitions of noise, I am implementing the noise as a random signal with 0 mean and a selectable standard deviation as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(a,std):\n",
    "    noise = np.random.normal(0,std,a._X.shape)\n",
    "    a._X = a._X + noise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b727167",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "std_levels = [0,0.25,0.5,0.75,1,1.25,1.5,1.75,2]\n",
    "\n",
    "for std in std_levels:\n",
    "    dataset = CustomDataset()\n",
    "    add_noise(dataset,std)\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size,val_size, test_size])\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "    accuracies.append(f1(test_loader,model_noiseless))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(np.arange(0,2.25, 0.25),accuracies)\n",
    "ax.set_ylim(ymin=0)\n",
    "plt.xlabel(\"Noise STD\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"LSTM - Accuracy vs Noise STD\")\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "out = model(inputs)\n",
    "print(out)\n",
    "for element in out:\n",
    "    print(torch.argmax(element))\n",
    "print(classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
