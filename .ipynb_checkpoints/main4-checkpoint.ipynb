{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce29de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a1486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetDrivers(Dataset):\n",
    "    def __init__(self, file_path='dataset.csv', classes_to_drop=['Class', \n",
    "                                                                 'PathOrder', \n",
    "                                                                 'Time(s)', \n",
    "                                                                 'Filtered_Accelerator_Pedal_value',\n",
    "                                                                 'Inhibition_of_engine_fuel_cut_off',\n",
    "                                                                 'Fuel_Pressure',\n",
    "                                                                 'Torque_scaling_factor(standardization)',\n",
    "                                                                 'Glow_plug_control_request'], window_size=16, normalize=True, normalize_method='mean_std'):\n",
    "        print(\"Loading\")\n",
    "        self._window_size=window_size\n",
    "        self._data=pd.read_csv(file_path)\n",
    "        self._data.sort_values(by=\"Class\", inplace=True)\n",
    "        print(\"Removing extra columns, initial data size is:\")\n",
    "        print(len(self._data))\n",
    "        n_per_class=[]\n",
    "        for class_uniq in list(self._data['Class'].unique()):\n",
    "            tot_number=sum(self._data['Class']==class_uniq)\n",
    "            n_per_class.append(tot_number-tot_number%window_size)\n",
    "            to_drop=tot_number%window_size\n",
    "            index_to_start_removing=self._data[self._data['Class']==class_uniq].index[0]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n",
    "        print(\"After removing, the size becomes:\")\n",
    "        \n",
    "        print(len(self._data))\n",
    "        self._data.reset_index(inplace=True)\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        index_starting_class=[]\n",
    "        #index_starting_class.append(0)\n",
    "        for class_uniq in list(self._data['Class'].unique()):\n",
    "            index_starting_class.append(self._data[self._data['Class']==class_uniq].index[0])\n",
    "        #import pdb; pdb.set_trace()\n",
    "        index_starting_class.append(len(self._data))\n",
    "        sequences=[]\n",
    "        for i in range(len(index_starting_class)):\n",
    "            if i!=len(index_starting_class)-1:#index_starting_class[-1]:\n",
    "                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
    "            \n",
    "            for j in range(0,len(ranges),int(self._window_size/2)):\n",
    "                if len(ranges[j:j+self._window_size])==16:\n",
    "                    sequences.append(ranges[j:j+self._window_size])\n",
    "        self._sequences=sequences\n",
    "        \n",
    "        print(\"Drop columns: \")\n",
    "        print(classes_to_drop)\n",
    "        self._labels=self._data['Class']\n",
    "        self._data.drop(classes_to_drop, inplace=True, axis=1)\n",
    "        if normalize:\n",
    "            for col in self._data.columns:\n",
    "                if normalize_method=='min_max':\n",
    "                    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())\n",
    "                elif normalize_method==\"mean_std\":\n",
    "                    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())\n",
    "        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n",
    "        y=[]#np.empty((len(sequences), ))\n",
    "        for n_row, sequence in enumerate(sequences):\n",
    "            X[n_row,:,:]=self._data.iloc[sequence]\n",
    "            y.append(self._labels.iloc[sequence])\n",
    "            #y.append(self._labels[sequence[0]])\n",
    "        assert len(y)==len(X)\n",
    "        self._X=X\n",
    "        \n",
    "        import pdb; pdb.set_trace()\n",
    "        targets = preprocessing.LabelEncoder().fit_transform(y)\n",
    "        targets = torch.as_tensor(targets)\n",
    "        #targets_tensor = torch.nn.functional.one_hot(targets.to(torch.int64), num_classes=10)\n",
    "        self._y=targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "781cd374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Removing extra columns, initial data size is:\n",
      "94380\n",
      "After removing, the size becomes:\n",
      "94320\n",
      "Drop columns: \n",
      "['Class', 'PathOrder', 'Time(s)', 'Filtered_Accelerator_Pedal_value', 'Inhibition_of_engine_fuel_cut_off', 'Fuel_Pressure', 'Torque_scaling_factor(standardization)', 'Glow_plug_control_request']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (12890, 16) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mDatasetDrivers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mDatasetDrivers.__init__\u001b[0;34m(self, file_path, classes_to_drop, window_size, normalize, normalize_method)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(X)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X\u001b[38;5;241m=\u001b[39mX\n\u001b[0;32m---> 70\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLabelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(targets)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#targets_tensor = torch.nn.functional.one_hot(targets.to(torch.int64), num_classes=10)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/binary_net/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/binary_net/lib/python3.9/site-packages/sklearn/utils/validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1153\u001b[0m         )\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1158\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (12890, 16) instead."
     ]
    }
   ],
   "source": [
    "a=DatasetDrivers(normalize_method =\"min_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04135d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(a))\n",
    "val_size = int(0.05 * len(a))\n",
    "test_size = len(a)-train_size-val_size#int(0.10 * len(a))\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(a, [train_size,val_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "n_workers=0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True,\n",
    "                                           num_workers=n_workers)\n",
    " \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False,\n",
    "                                           drop_last=True,\n",
    "                                           num_workers=n_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=n_workers)\n",
    "\n",
    "# The shape of the input element is batch,sequence,features\n",
    "# The input to the network must have batch_first = True\n",
    "\n",
    "class NEURAL(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, window_size, num_features):\n",
    "        super(NEURAL, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(num_features, 160,batch_first = True)\n",
    "        self.lstm2 = torch.nn.LSTM(160, 200, 1)\n",
    "        \n",
    "        # I need to get the output of last hidden layer \n",
    "        # What size is it DONT KNOW\n",
    "        self.fc = torch.nn.Linear(16, 10)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        lstm1_out, (h_t1, c_t1) = self.lstm1(x) \n",
    "        #print(\"shape of lstm1 is: \",lstm1_out.shape)\n",
    "        #print(\"shape of lstm1 h is: \",h_t1.shape)        \n",
    "        #print(\"shape of lstm1 c is: \",c_t1.shape)   \n",
    "        \n",
    "        lstm2_out, (h_t2, c_t2) = self.lstm2(lstm1_out)      \n",
    "        #print(\"shape of lstm2 is: \",lstm2_out.shape)\n",
    "        #print(\"shape of lstm2 h is: \",h_t2.shape)        \n",
    "        #print(\"shape of lstm2 c is: \",c_t2.shape)\n",
    "        #print(\"shape of lstm2 c is: \",h_t2[:,:,-1].shape)\n",
    "              \n",
    "        fc_out = self.fc(lstm2_out[:,:,-1])\n",
    "        out = self.sigmoid(fc_out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "inputs, classes = next(iter(train_loader)) \n",
    "\n",
    "model = NEURAL(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 500\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lambda1 = lambda epoch: 0.65 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8235c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 2.2753277683258055\n",
      "  batch 200 loss: 2.2765882849693297\n",
      "  batch 300 loss: 2.2682664799690246\n",
      "  batch 100 loss: 2.2664863753318785\n",
      "  batch 200 loss: 2.264766232967377\n",
      "  batch 300 loss: 2.2655766820907592\n",
      "  batch 100 loss: 2.264181535243988\n",
      "  batch 200 loss: 2.2546275949478147\n",
      "  batch 300 loss: 2.258261308670044\n",
      "  batch 100 loss: 2.2621767044067385\n",
      "  batch 200 loss: 2.248122823238373\n",
      "  batch 300 loss: 2.2475540471076965\n",
      "  batch 100 loss: 2.245270159244537\n",
      "  batch 200 loss: 2.245824098587036\n",
      "  batch 300 loss: 2.25107456445694\n",
      "  batch 100 loss: 2.2426262140274047\n",
      "  batch 200 loss: 2.2403342390060423\n",
      "  batch 300 loss: 2.235438549518585\n",
      "  batch 100 loss: 2.2327853226661682\n",
      "  batch 200 loss: 2.2277095484733582\n",
      "  batch 300 loss: 2.228038351535797\n",
      "  batch 100 loss: 2.222393605709076\n",
      "  batch 200 loss: 2.2146533703804017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 31\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs,labels)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/binary_net/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/binary_net/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(data)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        optimizer.step()\n",
    " \n",
    "        running_loss += loss.item()\n",
    "        #print(i)\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "    torch.cuda.empty_cache()\n",
    "    #scheduler.step()\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(train_loader)) \n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977774ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c863c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The shape of the input element is batch,sequence,features\n",
    "# The input to the network must have batch_first = True\n",
    "\n",
    "for it in train_loader:\n",
    "    print(np.shape(it[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "a._X[:10,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf82f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a._sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081511de",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=range(10)\n",
    "window=4\n",
    "s=[]\n",
    "for i in np.arange(0,len(indexes),int(window/2)):\n",
    "    s.append(indexes[i:i+window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158108c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_per_class=..\n",
    "n_per_class=tot_per_class/window/2\n",
    "X_array=np.empy((n_per_class, features, window))\n",
    "y_array=np.empty((n_per_class, ))\n",
    "\n",
    "indexes_starting_class=..\n",
    "for i in indexes_starting_class:\n",
    "    if i!=indexes_starting_class[-1]:\n",
    "        ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
    "    else:\n",
    "        ranges=np.arange(index_starting_class[i], len(data))\n",
    "    \n",
    "data.iloc[:10].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b83435",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=13\n",
    "sequences=...\n",
    "data_new=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3e78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
