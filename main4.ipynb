{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce29de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f73e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax=data.hist(bins=100, figsize=(36,18))\n",
    "plt.savefig(\"histogram.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.floor(155/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a._data['Filtered_Accelerator_Pedal_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(a._data.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetDrivers(Dataset):\n",
    "    def __init__(self, file_path='dataset.csv', classes_to_drop=['Class', \n",
    "                                                                 'PathOrder', \n",
    "                                                                 'Time(s)', \n",
    "                                                                 'Filtered_Accelerator_Pedal_value',\n",
    "                                                                 'Inhibition_of_engine_fuel_cut_off',\n",
    "                                                                 'Fuel_Pressure',\n",
    "                                                                 'Torque_scaling_factor(standardization)',\n",
    "                                                                 'Glow_plug_control_request'], window_size=16, normalize=True, normalize_method='mean_std'):\n",
    "        print(\"Loading\")\n",
    "        self._window_size=window_size\n",
    "        self._data=pd.read_csv(file_path)\n",
    "        self._data.sort_values(by=\"Class\", inplace=True)\n",
    "        print(\"Removing extra columns, initial data size is:\")\n",
    "        print(len(self._data))\n",
    "        n_per_class=[]\n",
    "        for class_uniq in list(self._data['Class'].unique()):\n",
    "            tot_number=sum(self._data['Class']==class_uniq)\n",
    "            n_per_class.append(tot_number-tot_number%window_size)\n",
    "            to_drop=tot_number%window_size\n",
    "            index_to_start_removing=self._data[self._data['Class']==class_uniq].index[0]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n",
    "        print(\"After removing, the size becomes:\")\n",
    "        print(len(self._data))\n",
    "        self._data.reset_index(inplace=True)\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        index_starting_class=[]\n",
    "        #index_starting_class.append(0)\n",
    "        for class_uniq in list(self._data['Class'].unique()):\n",
    "            index_starting_class.append(self._data[self._data['Class']==class_uniq].index[0])\n",
    "        #import pdb; pdb.set_trace()\n",
    "        index_starting_class.append(len(self._data))\n",
    "        sequences=[]\n",
    "        for i in range(len(index_starting_class)):\n",
    "            if i!=len(index_starting_class)-1:#index_starting_class[-1]:\n",
    "                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
    "            \n",
    "            for j in range(0,len(ranges),int(self._window_size/2)):\n",
    "                if len(ranges[j:j+self._window_size])==16:\n",
    "                    sequences.append(ranges[j:j+self._window_size])\n",
    "        self._sequences=sequences\n",
    "        \n",
    "        print(\"Drop columns: \")\n",
    "        print(classes_to_drop)\n",
    "        self._labels=self._data['Class'].values\n",
    "        self._data.drop(classes_to_drop, inplace=True, axis=1)\n",
    "        if normalize:\n",
    "            for col in self._data.columns:\n",
    "                if normalize_method=='min_max':\n",
    "                    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())\n",
    "                elif normalize_method==\"mean_std\":\n",
    "                    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())\n",
    "        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n",
    "        y=[]#np.empty((len(sequences), ))\n",
    "        for n_row, sequence in enumerate(sequences):\n",
    "            X[n_row,:,:]=self._data.iloc[sequence]\n",
    "            y.append(self._labels[sequence[0]])\n",
    "        assert len(y)==len(X)\n",
    "        self._X=X\n",
    "        targets = preprocessing.LabelEncoder().fit_transform(y)\n",
    "        targets = torch.as_tensor(targets)\n",
    "        #targets_tensor = torch.nn.functional.one_hot(targets.to(torch.int64), num_classes=10)\n",
    "        self._y=targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781cd374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Removing extra columns, initial data size is:\n",
      "94380\n",
      "After removing, the size becomes:\n",
      "94320\n",
      "Drop columns: \n",
      "['Class', 'PathOrder', 'Time(s)', 'Filtered_Accelerator_Pedal_value', 'Inhibition_of_engine_fuel_cut_off', 'Fuel_Pressure', 'Torque_scaling_factor(standardization)', 'Glow_plug_control_request']\n"
     ]
    }
   ],
   "source": [
    "a=DatasetDrivers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04135d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 2.3002632737159727\n",
      "  batch 200 loss: 2.294803659915924\n",
      "  batch 300 loss: 2.284939749240875\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 104>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m     vinputs, vlabels \u001b[38;5;241m=\u001b[39m vdata\n\u001b[0;32m    118\u001b[0m     voutputs \u001b[38;5;241m=\u001b[39m model(vinputs)\n\u001b[1;32m--> 119\u001b[0m     vloss \u001b[38;5;241m=\u001b[39m criterion(voutputs, \u001b[43mvlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    120\u001b[0m     running_vloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m vloss\n\u001b[0;32m    122\u001b[0m avg_vloss \u001b[38;5;241m=\u001b[39m running_vloss \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "train_size = int(0.85 * len(a))\n",
    "val_size = int(0.05 * len(a))\n",
    "test_size = len(a)-train_size-val_size#int(0.10 * len(a))\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(a, [train_size,val_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "n_workers=0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True,\n",
    "                                           num_workers=n_workers)\n",
    " \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False,\n",
    "                                           drop_last=True,\n",
    "                                           num_workers=n_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=n_workers)\n",
    "\n",
    "# The shape of the input element is batch,sequence,features\n",
    "# The input to the network must have batch_first = True\n",
    "\n",
    "class NEURAL(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, window_size, num_features):\n",
    "        super(NEURAL, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(num_features, 160,batch_first = True)\n",
    "        self.lstm2 = torch.nn.LSTM(160, 200, 1)\n",
    "        \n",
    "        # I need to get the output of last hidden layer \n",
    "        # What size is it DONT KNOW\n",
    "        self.fc = torch.nn.Linear(16, 10)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        lstm1_out, (h_t1, c_t1) = self.lstm1(x) \n",
    "        #print(\"shape of lstm1 is: \",lstm1_out.shape)\n",
    "        #print(\"shape of lstm1 h is: \",h_t1.shape)        \n",
    "        #print(\"shape of lstm1 c is: \",c_t1.shape)   \n",
    "        \n",
    "        lstm2_out, (h_t2, c_t2) = self.lstm2(lstm1_out)      \n",
    "        #print(\"shape of lstm2 is: \",lstm2_out.shape)\n",
    "        #print(\"shape of lstm2 h is: \",h_t2.shape)        \n",
    "        #print(\"shape of lstm2 c is: \",c_t2.shape)\n",
    "        #print(\"shape of lstm2 c is: \",h_t2[:,:,-1].shape)\n",
    "              \n",
    "        fc_out = self.fc(lstm2_out[:,:,-1])\n",
    "        out = self.sigmoid(fc_out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "inputs, classes = next(iter(train_loader)) \n",
    "\n",
    "model = NEURAL(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 500\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lambda1 = lambda epoch: 0.65 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #print(data)\n",
    "        #print(\"labels type\",type(labels))\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(data)\n",
    "        #print(\"the output has shape: \", outputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        optimizer.step()\n",
    " \n",
    "        running_loss += loss.item()\n",
    "        #print(i)\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = criterion(voutputs, vlabels[:,-1,:])\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with open('res.csv', 'a') as f:\n",
    "        f.write('{},{},{} \\n'.format(epoch_number + 1,avg_loss, avg_vloss))\n",
    "\n",
    "    print('{:d},{},{}'.format(epoch_number + 1,avg_loss, avg_vloss))\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(train_loader)) \n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977774ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c863c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The shape of the input element is batch,sequence,features\n",
    "# The input to the network must have batch_first = True\n",
    "\n",
    "for it in train_loader:\n",
    "    print(np.shape(it[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "a._X[:10,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf82f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a._sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081511de",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=range(10)\n",
    "window=4\n",
    "s=[]\n",
    "for i in np.arange(0,len(indexes),int(window/2)):\n",
    "    s.append(indexes[i:i+window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158108c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_per_class=..\n",
    "n_per_class=tot_per_class/window/2\n",
    "X_array=np.empy((n_per_class, features, window))\n",
    "y_array=np.empty((n_per_class, ))\n",
    "\n",
    "indexes_starting_class=..\n",
    "for i in indexes_starting_class:\n",
    "    if i!=indexes_starting_class[-1]:\n",
    "        ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
    "    else:\n",
    "        ranges=np.arange(index_starting_class[i], len(data))\n",
    "    \n",
    "data.iloc[:10].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b83435",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=13\n",
    "sequences=...\n",
    "data_new=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3e78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
